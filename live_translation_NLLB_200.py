import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel
from collections import deque
import time
import threading
import queue
import torch
from datetime import datetime, timedelta
import os
import warnings

# å¿½ç•¥æ— å…³è­¦å‘Š
warnings.filterwarnings("ignore")

class RealtimeMeetingTranscriber:
    def __init__(self, device_index=None, model_size="small", local_model_path=None):
        self.sample_rate = 16000
        self.device_index = device_index
        self.model_size = model_size
        self.local_model_path = local_model_path
        
        # æ–‡ä»¶å‘½å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.txt_file = f"meeting_{timestamp}.txt"
        self.srt_file = f"meeting_{timestamp}.srt"
        self.srt_counter = 1
        
        # 1. åˆå§‹åŒ– VAD (ä½¿ç”¨ CPU å³å¯ï¼ŒGPU åŠ é€Ÿæ”¶ç›Šæå°)
        print("æ­£åœ¨åŠ è½½ VAD æ¨¡å‹...")
        self.vad_model, utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False,
            onnx=False
        )
        (self.get_speech_timestamps, _, self.read_audio, *_) = utils
        print("âœ… VAD æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # 2. åˆå§‹åŒ– Whisper (Faster-Whisper åŸç”Ÿæ”¯æŒ GPU FP16)
        print(f"æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹ ({model_size})...")
        try:
            self.model = WhisperModel(model_size, device="cuda", compute_type="float16")
            print("âœ… Whisper: GPU (FP16) å°±ç»ª")
        except Exception as e:
            print(f"âŒ Whisper GPU åŠ è½½å¤±è´¥ï¼š{e}ï¼Œå°†ä½¿ç”¨ CPU")
            self.model = WhisperModel(model_size, device="cpu", compute_type="int8")
        
        # 3. åˆå§‹åŒ– NLLB ç¿»è¯‘æ¨¡å‹
        self.init_translator()
        
        # ç¼“å†²ä¸é˜Ÿåˆ—
        self.audio_buffer = deque()
        self.speech_buffer = []
        self.is_running = False
        self.audio_queue = queue.Queue()
        self.output_queue = queue.PriorityQueue()
        self.transcribe_counter = 0
        self.transcribe_lock = threading.Lock()
        
        # VAD å‚æ•° (é™ä½é™éŸ³é˜ˆå€¼ä»¥å‡å°‘æ–­å¥ç­‰å¾…æ—¶é—´)
        self.vad_chunk_size = 512
        self.min_silence_duration = 0.5  # ä» 0.6 æ”¹ä¸º 0.5 ç•¥å¾®é™ä½å»¶è¿Ÿ
        self.buffer_duration = 15.0      # å…è®¸æ›´é•¿çš„å•å¥ç¼“å†²
        self.max_buffer_samples = int(self.sample_rate * self.buffer_duration)
        self.last_speech_time = 0
        self.is_speaking = False
        self.cumulative_audio_duration = 0.0

    def init_translator(self):
        """å¼ºåˆ¶ GPU åŠ è½½ NLLB"""
        try:
            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
            
            print("æ­£åœ¨åŠ è½½ NLLB-200 ç¿»è¯‘æ¨¡å‹...")
            
            # æ˜¾å¼æŒ‡å®š CUDA
            self.device = torch.device("cuda")
            
            model_name = self.local_model_path if (self.local_model_path and os.path.exists(self.local_model_path)) else "facebook/nllb-200-distilled-600M"
            
            # åŠ è½½ Tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang="eng_Latn")
            
            # ã€å…³é”®ä¿®æ”¹ã€‘ç›´æ¥åŠ è½½åˆ° cudaï¼Œä¸ä½¿ç”¨ device_map é˜²æ­¢è¯¯åˆ¤
            # 12GB æ˜¾å­˜è¶³å¤Ÿç›´æ¥åŠ è½½ fp16 æ¨¡å‹
            self.translation_model = AutoModelForSeq2SeqLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16
            ).to(self.device)
            
            self.translation_model.eval()
            
            print(f"âœ… NLLB: GPU (FP16) å°±ç»ª | è®¾å¤‡ï¼š{self.translation_model.device}")
            
            # é¢„çƒ­ä¸€æ¬¡æ¨¡å‹ (æ¶ˆé™¤ç¬¬ä¸€æ¬¡æ¨ç†çš„å»¶è¿Ÿ)
            print("ğŸ”„ æ­£åœ¨é¢„çƒ­ç¿»è¯‘å¼•æ“...")
            self.translate_text("Hello world.")
            print("âš¡ ç³»ç»Ÿå‡†å¤‡å°±ç»ª")

        except Exception as e:
            print(f"âŒ NLLB åŠ è½½å¤±è´¥ï¼š{e}")
            raise

    def translate_text(self, text):
        """æé€Ÿç¿»è¯‘æ¨¡å¼"""
        try:
            if not text or not text.strip():
                return ""

            # 1. è®¾ç½®æºè¯­è¨€
            self.tokenizer.src_lang = "eng_Latn"
            
            # 2. ç¼–ç  (å¢åŠ  max_length é˜²æ­¢è¾“å…¥è¿‡é•¿è¢«æˆªæ–­)
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=1024
            ).to(self.device)
            
            forced_bos_token_id = self.tokenizer.convert_tokens_to_ids("zho_Hans")
            
            # 3. ç”Ÿæˆ (å…³é”®ä¼˜åŒ–å‚æ•°)
            with torch.no_grad():
                translated_tokens = self.translation_model.generate(
                    **inputs,
                    forced_bos_token_id=forced_bos_token_id,
                    max_new_tokens=512,  # ã€ä¿®å¤æˆªæ–­ã€‘ä½¿ç”¨ new_tokens ä¿è¯è¾“å‡ºå®Œæ•´
                    num_beams=1,         # ã€é™ä½å»¶è¿Ÿã€‘ä½¿ç”¨ Greedy Search (é€Ÿåº¦å¿« 5 å€)
                    do_sample=False,     # ç¡®å®šæ€§è¾“å‡ºï¼Œå‡å°‘è®¡ç®—é‡
                    repetition_penalty=1.1 # é˜²æ­¢å¶å°”çš„å¤è¯»æœºç°è±¡
                )
            
            # 4. è§£ç 
            translated_text = self.tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
            
            return self.convert_punctuation_to_chinese(translated_text)
            
        except Exception as e:
            print(f"âš ï¸ ç¿»è¯‘å¼‚å¸¸ï¼š{e}")
            return None

    def convert_punctuation_to_chinese(self, text):
        if not text: return text
        mapping = {',': 'ï¼Œ', '.': 'ã€‚', '!': 'ï¼', '?': 'ï¼Ÿ', ':': 'ï¼š', ';': 'ï¼›'}
        return "".join([mapping.get(c, c) for c in text])

    def format_srt_time(self, seconds):
        td = timedelta(seconds=seconds)
        hours, remainder = divmod(td.total_seconds(), 3600)
        minutes, seconds = divmod(remainder, 60)
        millis = int((seconds % 1) * 1000)
        return f"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d},{millis:03d}"

    def write_to_files(self, original_text, translated_text, detected_language, start_time, end_time):
        timestamp = datetime.now().strftime("%H:%M:%S")
        # å†™å…¥ TXT
        with open(self.txt_file, 'a', encoding='utf-8') as f:
            flag = "ğŸ‡¨ğŸ‡³" if detected_language == "zh" else "ğŸ‡¬ğŸ‡§"
            f.write(f"[{timestamp}] {flag} {original_text}\n")
            if translated_text:
                f.write(f"           âœ {translated_text}\n")
            f.write("\n")
        
        # å†™å…¥ SRT
        with open(self.srt_file, 'a', encoding='utf-8') as f:
            f.write(f"{self.srt_counter}\n")
            f.write(f"{self.format_srt_time(start_time)} --> {self.format_srt_time(end_time)}\n")
            f.write(f"{original_text}\n")
            if translated_text:
                f.write(f"{translated_text}\n")
            f.write("\n")
            self.srt_counter += 1

    def list_devices(self):
        print("\n=== å¯ç”¨çš„éŸ³é¢‘è®¾å¤‡ ===")
        print(sd.query_devices())

    def audio_callback(self, indata, frames, time_info, status):
        if status: print(status)
        self.audio_queue.put(indata[:, 0].copy())

    def process_audio(self):
        """éŸ³é¢‘å¤„ç†ä¸»å¾ªç¯"""
        while self.is_running:
            try:
                audio_chunk = self.audio_queue.get(timeout=0.1)
                self.audio_buffer.extend(audio_chunk)
                
                # VAD å¤„ç†
                while len(self.audio_buffer) >= self.vad_chunk_size:
                    chunk = np.array([self.audio_buffer.popleft() for _ in range(self.vad_chunk_size)])
                    
                    # è½¬æ¢å¼ é‡
                    chunk_tensor = torch.from_numpy(chunk).float().unsqueeze(0)
                    speech_prob = self.vad_model(chunk_tensor, self.sample_rate).item()
                    is_speech = speech_prob > 0.5
                    
                    current_time = time.time()
                    
                    if is_speech:
                        if not self.is_speaking:
                            self.is_speaking = True
                            self.speech_start_time = self.cumulative_audio_duration
                        self.speech_buffer.append(chunk)
                        self.last_speech_time = current_time
                    else:
                        if self.is_speaking:
                            # é™éŸ³è¶…è¿‡é˜ˆå€¼ï¼Œè§¦å‘è¯†åˆ«
                            if (current_time - self.last_speech_time) >= self.min_silence_duration:
                                self.trigger_transcription()
                    
                    # ç¼“å†²åŒºè¿‡å¤§å¼ºåˆ¶æˆªæ–­
                    if len(self.speech_buffer) * self.vad_chunk_size > self.max_buffer_samples:
                        self.trigger_transcription()
                        
                    self.cumulative_audio_duration += len(chunk) / self.sample_rate
                    
            except queue.Empty:
                continue

    def trigger_transcription(self):
        if len(self.speech_buffer) > 5: # å¿½ç•¥å¤ªçŸ­çš„æ‚éŸ³
            audio_segment = np.concatenate(self.speech_buffer)
            duration = len(audio_segment) / self.sample_rate
            
            # å¯åŠ¨çº¿ç¨‹è¿›è¡Œè¯†åˆ«ï¼Œä¸é˜»å¡å½•éŸ³
            threading.Thread(
                target=self.transcribe_task, 
                args=(audio_segment, self.speech_start_time, self.speech_start_time + duration), 
                daemon=True
            ).start()
            
        self.speech_buffer = []
        self.is_speaking = False

    def transcribe_task(self, audio_data, start_time, end_time):
        """è¯†åˆ« + ç¿»è¯‘ä»»åŠ¡"""
        try:
            audio_float32 = audio_data.astype(np.float32)
            
            # Whisper è¯†åˆ«
            segments, info = self.model.transcribe(
                audio_float32, 
                beam_size=1,        # å®æ—¶åœºæ™¯ beam_size=1 æ›´å¿«
                best_of=1,          # å‡å°‘å€™é€‰é‡‡æ ·
                vad_filter=True
            )
            
            original_text = "".join([s.text for s in segments]).strip()
            if not original_text: return

            # è·å– ID é”
            with self.transcribe_lock:
                transcribe_id = self.transcribe_counter
                self.transcribe_counter += 1

            translated_text = None
            if info.language != "zh":
                # è°ƒç”¨ç¿»è¯‘
                translated_text = self.translate_text(original_text)
            
            self.output_queue.put((transcribe_id, original_text, translated_text, info.language, start_time, end_time))
            
        except Exception as e:
            print(f"âŒ ä»»åŠ¡å‡ºé”™ï¼š{e}")

    def process_output(self):
        """è¾“å‡ºçº¿ç¨‹ï¼šç¡®ä¿æŒ‰é¡ºåºæ‰“å°"""
        next_id = 0
        pending_results = {} # æš‚å­˜ä¹±åºåˆ°è¾¾çš„ç»“æœ

        while not self.output_thread_stop:
            try:
                # å°è¯•è·å–ç»“æœ
                item = self.output_queue.get(timeout=0.1)
                tid, orig, trans, lang, start, end = item
                
                pending_results[tid] = (orig, trans, lang, start, end)
                
                # æŒ‰é¡ºåºå¤„ç†
                while next_id in pending_results:
                    orig, trans, lang, start, end = pending_results.pop(next_id)
                    
                    print(f"\nğŸ’¬ [{datetime.now().strftime('%H:%M:%S')}] {orig}")
                    if trans:
                        print(f"   âœ {trans}")
                    
                    self.write_to_files(orig, trans, lang, start, end)
                    next_id += 1
                    
            except queue.Empty:
                continue

    def start(self):
        self.is_running = True
        self.cumulative_audio_duration = 0.0
        
        # æ¸…ç©ºæ–‡ä»¶
        with open(self.txt_file, 'w', encoding='utf-8') as f:
            f.write(f"ä¼šè®®è®°å½• {datetime.now()}\n\n")
        open(self.srt_file, 'w').close()
        
        self.stream = sd.InputStream(
            device=self.device_index, channels=1, samplerate=self.sample_rate, 
            callback=self.audio_callback, blocksize=self.vad_chunk_size
        )
        self.stream.start()
        
        threading.Thread(target=self.process_audio, daemon=True).start()
        
        self.output_thread_stop = False
        self.output_thread = threading.Thread(target=self.process_output, daemon=True)
        self.output_thread.start()
        
        print(f"\nğŸš€ ç³»ç»Ÿå·²å¯åŠ¨ï¼(è®¾å¤‡ ID: {self.device_index if self.device_index else 'Default'})")
        print("è¯·è¯´è¯...")

    def stop(self):
        self.is_running = False
        self.output_thread_stop = True
        if hasattr(self, 'stream'): self.stream.stop()
        print("\nğŸ›‘ å·²åœæ­¢")

def main():
    app = RealtimeMeetingTranscriber()
    app.list_devices()
    try:
        idx = input("\nè¾“å…¥è®¾å¤‡ ID (å›è½¦é»˜è®¤): ").strip()
        if idx: app.device_index = int(idx)
        app.start()
        while True: time.sleep(1)
    except KeyboardInterrupt:
        app.stop()

if __name__ == "__main__":
    main()
