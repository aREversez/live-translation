import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel
from collections import deque
import time
import threading
import queue
import torch

class RealtimeMeetingTranscriber:
    def __init__(self, device_index=None, model_size="base", enable_translation=True):
        """
        å®æ—¶ä¼šè®®è½¬å½•å™¨ï¼ˆæ”¯æŒä¸­è‹±æ–‡æ··åˆï¼‰
        
        å‚æ•°:
        - device_index: éŸ³é¢‘è®¾å¤‡ç´¢å¼•ï¼ˆNone åˆ™ä½¿ç”¨é»˜è®¤ï¼‰
        - model_size: Whisper æ¨¡å‹å¤§å° (tiny/base/small/medium/large-v3)
        - enable_translation: æ˜¯å¦å¯ç”¨è‹±è¯‘ä¸­ç¿»è¯‘
        """
        self.sample_rate = 16000
        self.device_index = device_index
        self.enable_translation = enable_translation
        
        # åˆå§‹åŒ– Silero VAD
        print("æ­£åœ¨åŠ è½½ VAD æ¨¡å‹...")
        self.vad_model, utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False,
            onnx=False
        )
        (self.get_speech_timestamps, _, self.read_audio, *_) = utils
        
        # åˆå§‹åŒ– Whisper æ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹ï¼š{model_size}...")
        try:
            self.model = WhisperModel(model_size, device="cuda", compute_type="float16")
            print("ä½¿ç”¨ GPU åŠ é€Ÿ")
        except:
            self.model = WhisperModel(model_size, device="cpu", compute_type="int8")
            print("ä½¿ç”¨ CPU æ¨¡å¼")
        
        # åˆå§‹åŒ–ç¿»è¯‘æ¨¡å‹
        self.translator = None
        if enable_translation:
            self.init_translator()
        
        # éŸ³é¢‘ç¼“å†²
        self.audio_buffer = deque()
        self.speech_buffer = []
        
        # æ§åˆ¶å˜é‡
        self.is_running = False
        self.audio_queue = queue.Queue()
        self.text_queue = queue.Queue()
        
        # VAD å‚æ•°
        self.vad_chunk_size = 512
        self.min_silence_duration = 0.5
        self.buffer_duration = 10.0
        self.max_buffer_samples = int(self.sample_rate * self.buffer_duration)
        
        self.last_speech_time = 0
        self.is_speaking = False
    
    def init_translator(self):
        """åˆå§‹åŒ– Helsinki-NLP ç¿»è¯‘æ¨¡å‹"""
        try:
            from transformers import MarianMTModel, MarianTokenizer
            
            print("æ­£åœ¨åŠ è½½ç¿»è¯‘æ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½ï¼Œçº¦ 300MBï¼‰...")
            model_name = "Helsinki-NLP/opus-mt-en-zh"
            
            self.tokenizer = MarianTokenizer.from_pretrained(model_name)
            self.translation_model = MarianMTModel.from_pretrained(model_name)
            
            print("ç¿»è¯‘æ¨¡å‹åŠ è½½å®Œæˆ")
        
        except Exception as e:
            print(f"âš ï¸  åˆå§‹åŒ–ç¿»è¯‘æ¨¡å‹å¤±è´¥ï¼š{e}")
            self.enable_translation = False

    def translate_text(self, text):
        """ä½¿ç”¨ Helsinki-NLP ç¿»è¯‘"""
        if not self.enable_translation:
            return None
        
        try:
            # åˆ†å¥ç¿»è¯‘ï¼ˆé•¿æ–‡æœ¬æ•ˆæœæ›´å¥½ï¼‰
            sentences = text.split('. ')
            translated_sentences = []
            
            for sentence in sentences:
                if sentence.strip():
                    inputs = self.tokenizer(sentence, return_tensors="pt", padding=True)
                    translated = self.translation_model.generate(**inputs)
                    translated_text = self.tokenizer.decode(translated[0], skip_special_tokens=True)
                    translated_sentences.append(translated_text)
            
            return ''.join(translated_sentences)
            
        except Exception as e:
            print(f"   ç¿»è¯‘å‡ºé”™ï¼š{e}")
            return None
        
    def list_devices(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡"""
        print("\n=== å¯ç”¨çš„éŸ³é¢‘è®¾å¤‡ ===")
        devices = sd.query_devices()
        for i, device in enumerate(devices):
            if device['max_input_channels'] > 0:
                print(f"[{i}] {device['name']}")
                print(f"    è¾“å…¥é€šé“ï¼š{device['max_input_channels']}")
                print(f"    é»˜è®¤é‡‡æ ·ç‡ï¼š{device['default_samplerate']:.0f} Hz")
                print()
        return devices
    
    def audio_callback(self, indata, frames, time_info, status):
        """éŸ³é¢‘æµå›è°ƒå‡½æ•°"""
        if status:
            print(f"éŸ³é¢‘çŠ¶æ€ï¼š{status}")
        
        audio_data = indata[:, 0] if indata.shape[1] > 1 else indata.flatten()
        self.audio_queue.put(audio_data.copy())
    
    def detect_speech(self, audio_chunk):
        """ä½¿ç”¨ Silero VAD æ£€æµ‹è¯­éŸ³"""
        try:
            if len(audio_chunk) != self.vad_chunk_size:
                return False
            
            audio_tensor = torch.from_numpy(audio_chunk).float()
            audio_tensor = audio_tensor.unsqueeze(0)
            speech_prob = self.vad_model(audio_tensor, self.sample_rate).item()
            
            return speech_prob > 0.5
        except Exception as e:
            print(f"VAD æ£€æµ‹å‡ºé”™ï¼š{e}")
            return False
    
    def process_audio(self):
        """å¤„ç†éŸ³é¢‘æ•°æ®ï¼Œä½¿ç”¨ VAD æ£€æµ‹è¯­éŸ³ç‰‡æ®µ"""
        while self.is_running:
            try:
                audio_chunk = self.audio_queue.get(timeout=0.1)
                self.audio_buffer.extend(audio_chunk)
                
                while len(self.audio_buffer) >= self.vad_chunk_size:
                    chunk = np.array([self.audio_buffer.popleft() for _ in range(self.vad_chunk_size)])
                    
                    is_speech = self.detect_speech(chunk)
                    current_time = time.time()
                    
                    if is_speech:
                        if not self.is_speaking:
                            print("ğŸ¤ æ£€æµ‹åˆ°è¯­éŸ³...")
                            self.is_speaking = True
                        
                        self.speech_buffer.append(chunk)
                        self.last_speech_time = current_time
                        
                    else:
                        if self.is_speaking:
                            self.speech_buffer.append(chunk)
                            
                            silence_duration = current_time - self.last_speech_time
                            if silence_duration >= self.min_silence_duration:
                                if len(self.speech_buffer) > 5:
                                    audio_segment = np.concatenate(self.speech_buffer)
                                    threading.Thread(
                                        target=self.transcribe_audio, 
                                        args=(audio_segment,), 
                                        daemon=True
                                    ).start()
                                
                                self.speech_buffer = []
                                self.is_speaking = False
                        
                        if len(self.speech_buffer) * self.vad_chunk_size > self.max_buffer_samples:
                            if self.speech_buffer:
                                audio_segment = np.concatenate(self.speech_buffer)
                                threading.Thread(
                                    target=self.transcribe_audio, 
                                    args=(audio_segment,), 
                                    daemon=True
                                ).start()
                                self.speech_buffer = []
                                self.is_speaking = False
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"å¤„ç†éŸ³é¢‘å‡ºé”™ï¼š{e}")
    
    def is_chinese(self, text):
        """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸»è¦æ˜¯ä¸­æ–‡"""
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        total_chars = len([c for c in text if c.strip()])
        
        if total_chars == 0:
            return False
        
        return (chinese_chars / total_chars) > 0.3
    
    def transcribe_audio(self, audio_data):
        """ä½¿ç”¨ Whisper è½¬å½•éŸ³é¢‘ï¼Œè‡ªåŠ¨æ£€æµ‹è¯­è¨€å¹¶ç¿»è¯‘"""
        try:
            audio_float32 = audio_data.astype(np.float32)
            
            duration = len(audio_float32) / self.sample_rate
            if duration < 0.5:
                return
            
            print(f"ğŸ“ æ­£åœ¨è¯†åˆ«è¯­éŸ³ï¼ˆæ—¶é•¿ï¼š{duration:.1f}ç§’ï¼‰...")
            
            # è½¬å½•åŸæ–‡
            segments, info = self.model.transcribe(
                audio_float32,
                language=None,
                task="transcribe",
                beam_size=5,
                vad_filter=False,
                condition_on_previous_text=False
            )
            
            text_parts = []
            for segment in segments:
                text_parts.append(segment.text)
            
            if not text_parts:
                print("   (æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³)")
                return
            
            original_text = "".join(text_parts).strip()
            if not original_text:
                return
            
            detected_language = info.language
            language_probability = info.language_probability
            timestamp = time.strftime("%H:%M:%S")
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦ç¿»è¯‘
            if detected_language == "zh" or self.is_chinese(original_text):
                # ä¸­æ–‡ç›´æ¥è¾“å‡º
                result = f"[{timestamp}] ğŸ‡¨ğŸ‡³ {original_text}"
                print(f"\nâœ… {result}\n")
                self.text_queue.put(result)
            else:
                # è‹±æ–‡éœ€è¦ç¿»è¯‘
                lang_flag = "ğŸ‡¬ğŸ‡§" if detected_language == "en" else "ğŸŒ"
                
                if self.enable_translation:
                    print(f"   æ£€æµ‹åˆ°è¯­è¨€ï¼š{detected_language} (ç½®ä¿¡åº¦ï¼š{language_probability:.2f})ï¼Œæ­£åœ¨ç¿»è¯‘...")
                    translated_text = self.translate_text(original_text)
                    
                    if translated_text:
                        result = f"[{timestamp}] {lang_flag} {original_text}\n           âœ {translated_text}"
                        print(f"\nâœ… {result}\n")
                        self.text_queue.put(result)
                    else:
                        result = f"[{timestamp}] {lang_flag} {original_text}\n           âœ (ç¿»è¯‘å¤±è´¥)"
                        print(f"\nâœ… {result}\n")
                        self.text_queue.put(result)
                else:
                    # ä¸ç¿»è¯‘ï¼Œä»…æ˜¾ç¤ºåŸæ–‡
                    result = f"[{timestamp}] {lang_flag} {original_text}"
                    print(f"\nâœ… {result}\n")
                    self.text_queue.put(result)
        
        except Exception as e:
            print(f"è½¬å½•å‡ºé”™ï¼š{e}")
            import traceback
            traceback.print_exc()
    
    def start(self):
        """å¯åŠ¨å®æ—¶è½¬å½•"""
        self.is_running = True
        
        self.stream = sd.InputStream(
            device=self.device_index,
            channels=1,
            samplerate=self.sample_rate,
            callback=self.audio_callback,
            blocksize=self.vad_chunk_size
        )
        
        self.process_thread = threading.Thread(target=self.process_audio, daemon=True)
        self.process_thread.start()
        
        self.stream.start()
        print("\nâœ… å¼€å§‹å®æ—¶è½¬å½•ä¼šè®®éŸ³é¢‘...")
        print("ğŸ“ ä¸­æ–‡å°†ç›´æ¥æ˜¾ç¤ºï¼Œè‹±æ–‡å°†è‡ªåŠ¨ç¿»è¯‘æˆä¸­æ–‡")
        if self.enable_translation:
            print("ğŸŒ ç¿»è¯‘å¼•æ“ï¼šArgos Translate (æœ¬åœ°)")
        else:
            print("âš ï¸  ç¿»è¯‘å·²ç¦ç”¨ï¼Œä»…æ˜¾ç¤ºè‹±æ–‡åŸæ–‡")
        print("æŒ‰ Ctrl+C åœæ­¢\n")
    
    def stop(self):
        """åœæ­¢è½¬å½•"""
        self.is_running = False
        if hasattr(self, 'stream'):
            self.stream.stop()
            self.stream.close()
        if hasattr(self, 'process_thread'):
            self.process_thread.join(timeout=2)
        print("\nå·²åœæ­¢è½¬å½•")
    
    def get_transcriptions(self):
        """è·å–æ‰€æœ‰è½¬å½•ç»“æœ"""
        results = []
        while not self.text_queue.empty():
            results.append(self.text_queue.get())
        return results


def main():
    print("=" * 60)
    print("å®æ—¶ä¼šè®®è½¬å½•ç³»ç»Ÿ - ä¸­è‹±æ–‡æ··åˆæ”¯æŒï¼ˆå®Œå…¨ç¦»çº¿ï¼‰")
    print("=" * 60)
    
    # è¯¢é—®æ˜¯å¦å¯ç”¨ç¿»è¯‘
    enable_trans = input("\næ˜¯å¦å¯ç”¨è‹±è¯‘ä¸­ç¿»è¯‘ï¼Ÿ(y/n, é»˜è®¤ y): ").strip().lower()
    enable_translation = enable_trans != 'n'
    
    # åˆ›å»ºè½¬å½•å™¨
    transcriber = RealtimeMeetingTranscriber(
        model_size="base",
        enable_translation=enable_translation
    )
    
    # åˆ—å‡ºè®¾å¤‡
    transcriber.list_devices()
    
    # é€‰æ‹©è®¾å¤‡
    try:
        device_input = input("\nè¯·è¾“å…¥è¦ä½¿ç”¨çš„è®¾å¤‡ç¼–å·ï¼ˆç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤è®¾å¤‡ï¼‰: ").strip()
        device_id = int(device_input) if device_input else None
        transcriber.device_index = device_id
    except ValueError:
        print("ä½¿ç”¨é»˜è®¤è®¾å¤‡")
        transcriber.device_index = None
    
    # å¼€å§‹è½¬å½•
    transcriber.start()
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\n\næ­£åœ¨åœæ­¢...")
    finally:
        transcriber.stop()
        
        print("\n" + "=" * 60)
        print("ä¼šè®®è®°å½•")
        print("=" * 60)
        results = transcriber.get_transcriptions()
        for result in results:
            print(result)


if __name__ == "__main__":
    main()
