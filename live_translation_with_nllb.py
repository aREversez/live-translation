import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel
from collections import deque
import time
import threading
import queue
import torch
from datetime import datetime, timedelta
import os

class RealtimeMeetingTranscriber:
    def __init__(self, device_index=None, model_size="medium", local_model_path=None):
        """
        å®æ—¶ä¼šè®®è½¬å½•å™¨ï¼ˆæ”¯æŒä¸­è‹±æ–‡æ··åˆï¼‰
        
        å‚æ•°:
        - device_index: éŸ³é¢‘è®¾å¤‡ç´¢å¼•
        - model_size: Whisperæ¨¡å‹å¤§å° (base/small/medium/large-v3)
        - local_model_path: æœ¬åœ°NLLBæ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœå·²ä¸‹è½½ï¼‰
        """
        self.sample_rate = 16000
        self.device_index = device_index
        self.model_size = model_size
        self.local_model_path = local_model_path
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.txt_file = f"meeting_{timestamp}.txt"
        self.srt_file = f"meeting_{timestamp}.srt"
        
        # SRTå­—å¹•è®¡æ•°å™¨
        self.srt_counter = 1
        self.session_start_time = None
        
        # åˆå§‹åŒ–Silero VAD
        print("æ­£åœ¨åŠ è½½VADæ¨¡å‹...")
        self.vad_model, utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False,
            onnx=False
        )
        (self.get_speech_timestamps, _, self.read_audio, *_) = utils
        print("âœ… VADæ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åˆå§‹åŒ–Whisperæ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½Whisperæ¨¡å‹: {model_size}...")
        try:
            self.model = WhisperModel(model_size, device="cuda", compute_type="float16")
            print("âœ… Whisperä½¿ç”¨GPUåŠ é€Ÿ")
        except:
            self.model = WhisperModel(model_size, device="cpu", compute_type="int8")
            print("âœ… Whisperä½¿ç”¨CPUæ¨¡å¼")
        
        # åˆå§‹åŒ–ç¿»è¯‘æ¨¡å‹
        self.init_translator()
        
        # éŸ³é¢‘ç¼“å†²
        self.audio_buffer = deque()
        self.speech_buffer = []
        
        # æ§åˆ¶å˜é‡
        self.is_running = False
        self.audio_queue = queue.Queue()
        self.text_queue = queue.Queue()
        
        # VADå‚æ•°
        self.vad_chunk_size = 512
        self.min_silence_duration = 0.5
        self.buffer_duration = 10.0
        self.max_buffer_samples = int(self.sample_rate * self.buffer_duration)
        
        self.last_speech_time = 0
        self.is_speaking = False
        
        # è®°å½•ç´¯è®¡æ—¶é—´ï¼ˆç”¨äºSRTï¼‰
        self.cumulative_audio_duration = 0.0
    
    def init_translator(self):
        """åˆå§‹åŒ–NLLBç¿»è¯‘æ¨¡å‹"""
        try:
            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
            
            print("æ­£åœ¨åŠ è½½NLLB-200ç¿»è¯‘æ¨¡å‹...")
            
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹æˆ–ä»HuggingFaceä¸‹è½½
            if self.local_model_path and os.path.exists(self.local_model_path):
                print(f"ä»æœ¬åœ°åŠ è½½: {self.local_model_path}")
                model_name = self.local_model_path
            else:
                print("ä»HuggingFaceä¸‹è½½ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½çº¦600MBï¼‰...")
                model_name = "facebook/nllb-200-distilled-600M"
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang="eng_Latn")
            self.translation_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            self.target_lang = "zho_Hans"  # ç®€ä½“ä¸­æ–‡
            
            # å¦‚æœæœ‰GPUï¼Œå°†æ¨¡å‹ç§»åˆ°GPU
            if torch.cuda.is_available():
                self.translation_model = self.translation_model.cuda()
                print("âœ… ç¿»è¯‘æ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ")
            else:
                print("âœ… ç¿»è¯‘æ¨¡å‹ä½¿ç”¨CPU")
            
            print("âœ… NLLBç¿»è¯‘æ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–ç¿»è¯‘æ¨¡å‹å¤±è´¥: {e}")
            print("\nå¯èƒ½çš„åŸå› :")
            print("1. é¦–æ¬¡ä½¿ç”¨éœ€è¦è”ç½‘ä¸‹è½½æ¨¡å‹ï¼ˆçº¦600MBï¼‰")
            print("2. ç£ç›˜ç©ºé—´ä¸è¶³")
            print("3. ç¼ºå°‘ä¾èµ–: pip install transformers sentencepiece sacremoses")
            import traceback
            traceback.print_exc()
            raise
    
    def translate_text(self, text):
        """ä½¿ç”¨NLLBç¿»è¯‘æ–‡æœ¬ï¼ˆè‹±æ–‡åˆ°ä¸­æ–‡ï¼‰"""
        try:
            inputs = self.tokenizer(text, return_tensors="pt", padding=True, max_length=512, truncation=True)
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            translated_tokens = self.translation_model.generate(
                **inputs,
                forced_bos_token_id=self.tokenizer.lang_code_to_id[self.target_lang],
                max_length=512,
                num_beams=5,
                early_stopping=True
            )
            translated_text = self.tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
            
            return translated_text
            
        except Exception as e:
            print(f"   ç¿»è¯‘å‡ºé”™: {e}")
            return None
    
    def format_srt_time(self, seconds):
        """æ ¼å¼åŒ–SRTæ—¶é—´æˆ³"""
        td = timedelta(seconds=seconds)
        hours = int(td.total_seconds() // 3600)
        minutes = int((td.total_seconds() % 3600) // 60)
        secs = int(td.total_seconds() % 60)
        millis = int((td.total_seconds() % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"
    
    def write_to_files(self, original_text, translated_text, detected_language, start_time, end_time):
        """å†™å…¥TXTå’ŒSRTæ–‡ä»¶"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        
        # å†™å…¥TXTæ–‡ä»¶
        with open(self.txt_file, 'a', encoding='utf-8') as f:
            if detected_language == "zh":
                f.write(f"[{timestamp}] ğŸ‡¨ğŸ‡³ {original_text}\n")
            else:
                lang_flag = "ğŸ‡¬ğŸ‡§" if detected_language == "en" else "ğŸŒ"
                f.write(f"[{timestamp}] {lang_flag} {original_text}\n")
                if translated_text:
                    f.write(f"           âœ {translated_text}\n")
            f.write("\n")
        
        # å†™å…¥SRTæ–‡ä»¶
        with open(self.srt_file, 'a', encoding='utf-8') as f:
            f.write(f"{self.srt_counter}\n")
            f.write(f"{self.format_srt_time(start_time)} --> {self.format_srt_time(end_time)}\n")
            
            if detected_language == "zh":
                f.write(f"{original_text}\n")
            else:
                f.write(f"{original_text}\n")
                if translated_text:
                    f.write(f"{translated_text}\n")
            
            f.write("\n")
            self.srt_counter += 1
        
    def list_devices(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡"""
        print("\n=== å¯ç”¨çš„éŸ³é¢‘è®¾å¤‡ ===")
        devices = sd.query_devices()
        for i, device in enumerate(devices):
            if device['max_input_channels'] > 0:
                print(f"[{i}] {device['name']}")
                print(f"    è¾“å…¥é€šé“: {device['max_input_channels']}")
                print(f"    é»˜è®¤é‡‡æ ·ç‡: {device['default_samplerate']:.0f} Hz")
                print()
        return devices
    
    def audio_callback(self, indata, frames, time_info, status):
        """éŸ³é¢‘æµå›è°ƒå‡½æ•°"""
        if status:
            print(f"éŸ³é¢‘çŠ¶æ€: {status}")
        
        audio_data = indata[:, 0] if indata.shape[1] > 1 else indata.flatten()
        self.audio_queue.put(audio_data.copy())
    
    def detect_speech(self, audio_chunk):
        """ä½¿ç”¨Silero VADæ£€æµ‹è¯­éŸ³"""
        try:
            if len(audio_chunk) != self.vad_chunk_size:
                return False
            
            audio_tensor = torch.from_numpy(audio_chunk).float()
            audio_tensor = audio_tensor.unsqueeze(0)
            speech_prob = self.vad_model(audio_tensor, self.sample_rate).item()
            
            return speech_prob > 0.5
        except Exception as e:
            print(f"VADæ£€æµ‹å‡ºé”™: {e}")
            return False
    
    def process_audio(self):
        """å¤„ç†éŸ³é¢‘æ•°æ®ï¼Œä½¿ç”¨VADæ£€æµ‹è¯­éŸ³ç‰‡æ®µ"""
        while self.is_running:
            try:
                audio_chunk = self.audio_queue.get(timeout=0.1)
                self.audio_buffer.extend(audio_chunk)
                
                while len(self.audio_buffer) >= self.vad_chunk_size:
                    chunk = np.array([self.audio_buffer.popleft() for _ in range(self.vad_chunk_size)])
                    
                    is_speech = self.detect_speech(chunk)
                    current_time = time.time()
                    
                    if is_speech:
                        if not self.is_speaking:
                            print("ğŸ¤ æ£€æµ‹åˆ°è¯­éŸ³...")
                            self.is_speaking = True
                            self.speech_start_time = self.cumulative_audio_duration
                        
                        self.speech_buffer.append(chunk)
                        self.last_speech_time = current_time
                        
                    else:
                        if self.is_speaking:
                            self.speech_buffer.append(chunk)
                            
                            silence_duration = current_time - self.last_speech_time
                            if silence_duration >= self.min_silence_duration:
                                if len(self.speech_buffer) > 5:
                                    audio_segment = np.concatenate(self.speech_buffer)
                                    segment_duration = len(audio_segment) / self.sample_rate
                                    
                                    threading.Thread(
                                        target=self.transcribe_audio, 
                                        args=(audio_segment, self.speech_start_time, 
                                              self.speech_start_time + segment_duration), 
                                        daemon=True
                                    ).start()
                                
                                self.speech_buffer = []
                                self.is_speaking = False
                        
                        if len(self.speech_buffer) * self.vad_chunk_size > self.max_buffer_samples:
                            if self.speech_buffer:
                                audio_segment = np.concatenate(self.speech_buffer)
                                segment_duration = len(audio_segment) / self.sample_rate
                                
                                threading.Thread(
                                    target=self.transcribe_audio, 
                                    args=(audio_segment, self.speech_start_time,
                                          self.speech_start_time + segment_duration), 
                                    daemon=True
                                ).start()
                                self.speech_buffer = []
                                self.is_speaking = False
                    
                    # æ›´æ–°ç´¯è®¡æ—¶é—´
                    self.cumulative_audio_duration += len(chunk) / self.sample_rate
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"å¤„ç†éŸ³é¢‘å‡ºé”™: {e}")
    
    def is_chinese(self, text):
        """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸»è¦æ˜¯ä¸­æ–‡"""
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        total_chars = len([c for c in text if c.strip()])
        
        if total_chars == 0:
            return False
        
        return (chinese_chars / total_chars) > 0.3
    
    def transcribe_audio(self, audio_data, start_time, end_time):
        """ä½¿ç”¨Whisperè½¬å½•éŸ³é¢‘ï¼Œè‡ªåŠ¨æ£€æµ‹è¯­è¨€å¹¶ç¿»è¯‘"""
        try:
            audio_float32 = audio_data.astype(np.float32)
            
            duration = len(audio_float32) / self.sample_rate
            if duration < 0.5:
                return
            
            print(f"ğŸ“ æ­£åœ¨è¯†åˆ«è¯­éŸ³ï¼ˆæ—¶é•¿: {duration:.1f}ç§’ï¼‰...")
            
            # è½¬å½•åŸæ–‡
            segments, info = self.model.transcribe(
                audio_float32,
                language=None,
                task="transcribe",
                beam_size=5,
                vad_filter=False,
                condition_on_previous_text=False
            )
            
            text_parts = []
            for segment in segments:
                text_parts.append(segment.text)
            
            if not text_parts:
                print("   (æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³)")
                return
            
            original_text = "".join(text_parts).strip()
            if not original_text:
                return
            
            detected_language = info.language
            language_probability = info.language_probability
            timestamp = datetime.now().strftime("%H:%M:%S")
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦ç¿»è¯‘
            if detected_language == "zh" or self.is_chinese(original_text):
                # ä¸­æ–‡ç›´æ¥è¾“å‡º
                result = f"[{timestamp}] ğŸ‡¨ğŸ‡³ {original_text}"
                print(f"\nâœ… {result}\n")
                
                # å†™å…¥æ–‡ä»¶
                self.write_to_files(original_text, None, "zh", start_time, end_time)
                
            else:
                # è‹±æ–‡éœ€è¦ç¿»è¯‘
                lang_flag = "ğŸ‡¬ğŸ‡§" if detected_language == "en" else "ğŸŒ"
                print(f"   æ£€æµ‹åˆ°è¯­è¨€: {detected_language} (ç½®ä¿¡åº¦: {language_probability:.2f})ï¼Œæ­£åœ¨ç¿»è¯‘...")
                
                translated_text = self.translate_text(original_text)
                
                if translated_text:
                    result = f"[{timestamp}] {lang_flag} {original_text}\n           âœ {translated_text}"
                    print(f"\nâœ… {result}\n")
                    
                    # å†™å…¥æ–‡ä»¶
                    self.write_to_files(original_text, translated_text, detected_language, start_time, end_time)
                else:
                    result = f"[{timestamp}] {lang_flag} {original_text}\n           âœ (ç¿»è¯‘å¤±è´¥)"
                    print(f"\nâœ… {result}\n")
                    
                    # å†™å…¥æ–‡ä»¶ï¼ˆæ— ç¿»è¯‘ï¼‰
                    self.write_to_files(original_text, None, detected_language, start_time, end_time)
        
        except Exception as e:
            print(f"è½¬å½•å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def start(self):
        """å¯åŠ¨å®æ—¶è½¬å½•"""
        self.is_running = True
        self.session_start_time = time.time()
        self.cumulative_audio_duration = 0.0
        
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
        with open(self.txt_file, 'w', encoding='utf-8') as f:
            f.write(f"ä¼šè®®è®°å½• - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 60 + "\n\n")
        
        with open(self.srt_file, 'w', encoding='utf-8') as f:
            pass  # åˆ›å»ºç©ºæ–‡ä»¶
        
        self.stream = sd.InputStream(
            device=self.device_index,
            channels=1,
            samplerate=self.sample_rate,
            callback=self.audio_callback,
            blocksize=self.vad_chunk_size
        )
        
        self.process_thread = threading.Thread(target=self.process_audio, daemon=True)
        self.process_thread.start()
        
        self.stream.start()
        print("\n" + "=" * 60)
        print("âœ… å¼€å§‹å®æ—¶è½¬å½•ä¼šè®®éŸ³é¢‘")
        print("=" * 60)
        print(f"ğŸ“ æ–‡æœ¬è®°å½•: {self.txt_file}")
        print(f"ğŸ“º å­—å¹•æ–‡ä»¶: {self.srt_file}")
        print(f"ğŸ¤– Whisperæ¨¡å‹: {self.model_size}")
        print(f"ğŸŒ ç¿»è¯‘æ¨¡å‹: NLLB-200")
        print("=" * 60)
        print("æŒ‰ Ctrl+C åœæ­¢\n")
    
    def stop(self):
        """åœæ­¢è½¬å½•"""
        self.is_running = False
        if hasattr(self, 'stream'):
            self.stream.stop()
            self.stream.close()
        if hasattr(self, 'process_thread'):
            self.process_thread.join(timeout=2)
        
        print("\n" + "=" * 60)
        print("âœ… è½¬å½•å·²åœæ­¢")
        print(f"ğŸ“ æ–‡æœ¬è®°å½•å·²ä¿å­˜: {self.txt_file}")
        print(f"ğŸ“º å­—å¹•æ–‡ä»¶å·²ä¿å­˜: {self.srt_file}")
        print("=" * 60)


def main():
    print("=" * 60)
    print("å®æ—¶ä¼šè®®è½¬å½•ç³»ç»Ÿ - å®Œå…¨ç¦»çº¿ç‰ˆ")
    print("åŠŸèƒ½: ä¸­è‹±æ–‡è¯†åˆ« + è‡ªåŠ¨ç¿»è¯‘ + TXT/SRTè¾“å‡º")
    print("=" * 60)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°æ¨¡å‹
    local_model = None
    if os.path.exists("./nllb-model"):
        print("\nâœ… æ£€æµ‹åˆ°æœ¬åœ°NLLBæ¨¡å‹")
        local_model = "./nllb-model"
    
    # åˆ›å»ºè½¬å½•å™¨
    transcriber = RealtimeMeetingTranscriber(
        model_size="medium",
        local_model_path=local_model
    )
    
    # åˆ—å‡ºè®¾å¤‡
    transcriber.list_devices()
    
    # é€‰æ‹©è®¾å¤‡
    try:
        device_input = input("\nè¯·è¾“å…¥è¦ä½¿ç”¨çš„è®¾å¤‡ç¼–å·ï¼ˆç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤è®¾å¤‡ï¼‰: ").strip()
        device_id = int(device_input) if device_input else None
        transcriber.device_index = device_id
    except ValueError:
        print("ä½¿ç”¨é»˜è®¤è®¾å¤‡")
        transcriber.device_index = None
    
    # å¼€å§‹è½¬å½•
    transcriber.start()
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\n\næ­£åœ¨åœæ­¢...")
    finally:
        transcriber.stop()


if __name__ == "__main__":
    main()
